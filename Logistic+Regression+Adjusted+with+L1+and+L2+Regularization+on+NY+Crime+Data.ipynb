{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building a maultivariable regressor using crime data in New York State in 2013(Source: FBI, UCR.)\n",
    "We try to predict occurrence of murder in any city using other crime related data</h2>\n",
    "\n",
    "To reduce the risk of overfitting we will apply following regularization procedures for the linear regression:\n",
    "\n",
    "- Ridge Regression: where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).\n",
    "- Lasso Regression: where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Use regular OLS and try to improve it by L1 L2 procedures\n",
    "## Part 2: Use Logistic Regression and try to improve it by L1 L2 procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab and process the cleaned data\n",
    "path1 = (\"C:/Users/aath/Dropbox/MAEN/Thankful/Data/NY Crime/NY_Crime_2013_cleaned2.csv\")\n",
    "\n",
    "df = pd.read_csv(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>prop</th>\n",
       "      <th>pop</th>\n",
       "      <th>murd</th>\n",
       "      <th>rob</th>\n",
       "      <th>violent</th>\n",
       "      <th>assul</th>\n",
       "      <th>burg</th>\n",
       "      <th>theft</th>\n",
       "      <th>motor</th>\n",
       "      <th>murd_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adams Village</td>\n",
       "      <td>11</td>\n",
       "      <td>1851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Addison Town and Village</td>\n",
       "      <td>49</td>\n",
       "      <td>2568</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afton Village4</td>\n",
       "      <td>1</td>\n",
       "      <td>820</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron Village</td>\n",
       "      <td>17</td>\n",
       "      <td>2842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany4</td>\n",
       "      <td>3888</td>\n",
       "      <td>98595</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>802</td>\n",
       "      <td>503</td>\n",
       "      <td>683</td>\n",
       "      <td>3083</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       City  prop    pop  murd  rob  violent  assul  burg  \\\n",
       "0             Adams Village    11   1851     0    0        0      0     1   \n",
       "1  Addison Town and Village    49   2568     0    1        2      1     1   \n",
       "2            Afton Village4     1    820     0    0        0      0     0   \n",
       "3             Akron Village    17   2842     0    0        1      1     0   \n",
       "4                   Albany4  3888  98595     8  237      802    503   683   \n",
       "\n",
       "   theft  motor  murd_binary  \n",
       "0     10      0            0  \n",
       "1     47      1            0  \n",
       "2      1      0            0  \n",
       "3     17      0            0  \n",
       "4   3083    122            1  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Remove New York City as it is the largest city in NY State and does distort visualization\n",
    "df = df[df.City != 'New York']\n",
    "\n",
    "# Murder = murd is our dependent binary variable already represented by murd_binayr. \n",
    "# City is not \n",
    "df = df.drop('murd',1)\n",
    "df = df.drop('City',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prop           0\n",
       "pop            0\n",
       "rob            0\n",
       "violent        0\n",
       "assul          0\n",
       "burg           0\n",
       "theft          0\n",
       "motor          0\n",
       "murd_binary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trainsize = int(df.shape[0] / 2)\n",
    "chosen_idx1 = np.random.choice(df.shape[0], replace=False, size=trainsize)\n",
    "chosen_idx2 = np.random.choice(df.shape[0], replace=False, size=trainsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen_idx2 = chosen_idx1[~chosen_idx1.index.isin(chosen_idx1.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "0.370176275746\n"
     ]
    }
   ],
   "source": [
    "# Define the training and test sizes.\n",
    "df.sample(frac=1) # Shuffle the data\n",
    "trainsize = int(df.shape[0] / 2)\n",
    "df_test = df.iloc[trainsize:, :].copy()\n",
    "df_train = df.iloc[:trainsize, :].copy()\n",
    "\n",
    "# Set up the regression model to predict defaults using all other\n",
    "# variables as features.\n",
    "regr1 = linear_model.LinearRegression()\n",
    "Y_train = df_train['murd_binary'].values.reshape(-1, 1)\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['murd_binary'])]\n",
    "regr1.fit(X_train, Y_train)\n",
    "print('\\nR-squared simple model:')\n",
    "print(regr1.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model explains 37% of variability of the response data around the mean.\n",
    "- R-squared = Explained variation / Total variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.42384974e-03,   5.50090802e-06,  -6.38857371e-03,\n",
       "         9.52125573e-03,  -9.31481865e-03,  -2.31074785e-04,\n",
       "         1.72793299e-03,  -2.92070794e-03,   1.97542960e-02])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the parameter estimates.\n",
    "origparams = np.append(regr1.coef_, regr1.intercept_)\n",
    "origparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make new features to capture potential quadratic\n",
    "# between the features.\n",
    "\n",
    "df_train['prop2'] = (df_train['prop'] + 100) ** 2\n",
    "df_train['pop2'] = (df_train['pop'] + 100) ** 2\n",
    "df_train['rob2'] = (df_train['rob'] + 100) ** 2\n",
    "df_train['violent2'] = (df_train['violent'] + 100) ** 2\n",
    "df_train['assul2'] = (df_train['assul'] + 100) ** 2\n",
    "df_train['burg2'] = (df_train['burg'] + 100) ** 2\n",
    "df_train['theft2'] = (df_train['theft'] + 100) ** 2\n",
    "df_train['mortor2'] = (df_train['motor'] + 100) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared complex model:\n",
      "0.528070657814\n"
     ]
    }
   ],
   "source": [
    "# Re-run the model with the new features.\n",
    "regrBig = linear_model.LinearRegression()\n",
    "X_train2 = df_train.loc[:, ~(df_train.columns).isin(['murd_binary'])]\n",
    "regrBig.fit(X_train2, Y_train)\n",
    "print('\\nR-squared complex model:')\n",
    "print(regrBig.score(X_train2, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- R-squared increases by added features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter Estimates for the same predictors for the small model and large model:\n",
      "[[-0.001 -0.016]\n",
      " [0.000 0.000]\n",
      " [-0.006 0.030]\n",
      " [0.010 0.009]\n",
      " [-0.009 -0.010]\n",
      " [-0.000 0.013]\n",
      " [0.002 0.016]\n",
      " [-0.003 -0.045]\n",
      " [0.020 -1.935]]\n"
     ]
    }
   ],
   "source": [
    "# Store the new parameter estimates for the same features.\n",
    "newparams = np.append(\n",
    "    regrBig.coef_[0,0:(len(origparams)-1)],\n",
    "    regrBig.intercept_)\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model '\n",
    "      'and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After adding extra features, the magnitute and sign of some of coefficients changed. This can be a sign of overfitting and of having too many correlated dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "-468.526557014\n",
      "\n",
      "R-squared complex model:\n",
      "-13997302.8083\n"
     ]
    }
   ],
   "source": [
    "# Test the simpler model with smaller coefficients.\n",
    "Y_test = df_test['murd_binary'].values.reshape(-1, 1)\n",
    "X_test = df_test.loc[:, ~(df_test.columns).isin(['murd_binary'])]\n",
    "print('\\nR-squared simple model:')\n",
    "print(regr1.score(X_test, Y_test))\n",
    "\n",
    "# Test the more complex model with larger coefficients.\n",
    "\n",
    "df_test['prop2'] = (df_test['prop'] + 100) ** 2\n",
    "df_test['pop2'] = (df_test['pop'] + 100) ** 2\n",
    "df_test['rob2'] = (df_test['rob'] + 100) ** 2\n",
    "df_test['violent2'] = (df_test['violent'] + 100) ** 2\n",
    "df_test['assul2'] = (df_test['assul'] + 100) ** 2\n",
    "df_test['burg2'] = (df_test['burg'] + 100) ** 2\n",
    "df_test['theft2'] = (df_test['theft'] + 100) ** 2\n",
    "df_test['mortor2'] = (df_test['motor'] + 100) ** 2\n",
    "\n",
    "# Re-run the model with the new features.\n",
    "X_test2 = df_test.loc[:, ~(df_test.columns).isin(['murd_binary'])]\n",
    "print('\\nR-squared complex model:')\n",
    "print(regrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing the model on remaining data set result in a very poor outcome. A negative R-squared indicates that the fit of the model is even worse than a horizontal line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we try to improve the model by using Ridge procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.367976300523\n",
      "[ -1.35481010e-03   6.50818947e-06  -6.06347569e-03   8.63844577e-03\n",
      "  -8.20213564e-03  -1.62541232e-04   1.62321631e-03  -2.81548517e-03]\n",
      "0.519280758028\n",
      "\n",
      "Parameter Estimates for the same predictors for the small model and large model:\n",
      "[[-0.001 -0.007]\n",
      " [0.000 0.000]\n",
      " [-0.006 0.032]\n",
      " [0.009 -0.006]\n",
      " [-0.008 0.015]\n",
      " [-0.000 0.002]\n",
      " [0.002 0.007]\n",
      " [-0.003 -0.016]]\n"
     ]
    }
   ],
   "source": [
    "# To address the overfitting problem we can neglect the requirement of an unbiased parameter estimator \n",
    "# and instead use a biased estimator, which may have smaller variance.\n",
    "# One of such biased estimators is the Ridge method. \n",
    "# Below we will fit a ridge regression model where alpha is the regularization parameter (usually called lambda). \n",
    "# As alpha gets larger, parameter shrinkage grows more pronounced. Note that by convention, the intercept is not regularized. \n",
    "\n",
    "ridgeregr = linear_model.Ridge(alpha=20, fit_intercept=False) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "print(ridgeregr.score(X_train, Y_train))\n",
    "origparams = ridgeregr.coef_[0]\n",
    "print(origparams)\n",
    "\n",
    "ridgeregrBig = linear_model.Ridge(alpha=20, fit_intercept=False)\n",
    "ridgeregrBig.fit(X_train2, Y_train)\n",
    "print(ridgeregrBig.score(X_train2, Y_train))\n",
    "newparams = ridgeregrBig.coef_[0, 0:len(origparams)]\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model'\n",
    "      ' and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "-504.075019864\n",
      "\n",
      "R-squared complex model:\n",
      "-16245882.5219\n"
     ]
    }
   ],
   "source": [
    "print('\\nR-squared simple model:')\n",
    "print(ridgeregr.score(X_test, Y_test))\n",
    "print('\\nR-squared complex model:')\n",
    "print(ridgeregrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Risdge regression using regularization parameter λ = 20 make the model worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we try to improve the model by using Lasso procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we will use Lasso Regression which is a close cousin of Ridge Regression, in which absolute values of coefficients are minimized rather than the square of values. This method will help to eliminate some insignificant variables. Lasso regression is generally used when there is a very large number of variables, since Lasso automatically does the variables selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for the model with few features:\n",
      "0.348956285342\n",
      "\n",
      "Parameter estimates for the model with few features:\n",
      "[ -0.00000000e+00   4.77910101e-06  -0.00000000e+00   8.73183121e-05\n",
      "   0.00000000e+00  -9.72696778e-04   3.33892171e-04  -0.00000000e+00\n",
      "   1.51999064e-02]\n",
      "\n",
      "R² for the model with many features:\n",
      "0.46285409656\n",
      "\n",
      "Parameter estimates for the model with many features:\n",
      "[  4.50473213e-04  -2.60139952e-09   0.00000000e+00   1.15178047e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -9.07274967e-08   2.94768075e-11  -7.03863650e-07  -1.93179870e-07\n",
      "  -3.28346609e-07   5.55158235e-07  -5.68330024e-08   6.01014665e-06\n",
      "  -5.94734578e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Small number of parameters.\n",
    "lass = linear_model.Lasso(alpha=.35)\n",
    "lassfit = lass.fit(X_train, Y_train)\n",
    "print('R² for the model with few features:')\n",
    "print(lass.score(X_train, Y_train))\n",
    "origparams = np.append(lassfit.coef_, lassfit.intercept_)\n",
    "print('\\nParameter estimates for the model with few features:')\n",
    "print(origparams)\n",
    "\n",
    "# Large number of parameters.\n",
    "lassBig = linear_model.Lasso(alpha=.35)\n",
    "lassBig.fit(X_train2, Y_train)\n",
    "print('\\nR² for the model with many features:')\n",
    "print(lassBig.score(X_train2, Y_train))\n",
    "origparams = np.append(lassBig.coef_, lassBig.intercept_)\n",
    "print('\\nParameter estimates for the model with many features:')\n",
    "print(origparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "-194.185555919\n",
      "\n",
      "R-squared complex model:\n",
      "-19706.5628076\n"
     ]
    }
   ],
   "source": [
    "# Checking predictive power using the test set:\n",
    "print('\\nR-squared simple model:')\n",
    "print(lass.score(X_test, Y_test))\n",
    "print('\\nR-squared complex model:')\n",
    "print(lassBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the Lasso method the model improves considerably compared to previous models. However, because of negative R-squared we can conclude that this model, with its constraints, fits the data really poorly.\n",
    "- We can also see that the Lasso method is able to make a selection of variables. Overall we can say that both Lasso and Ridge balance the trade-off bias-variance with the choice of lambda. On this dataset the Lasso performed better and apparently it was because of many of predators were not actually tied to the response variable. And to test this one should use cross-validation and also try to do an stepwise regression. Here we started with all variables and tried to reduce or shrink them. Another approach is to go forward stepwise by starting with an empty model and choose the variables with most significant association. In each step we add variable that can provide statistically significant association with the dependent variable. By using cross-validation and pipeline module we should be able to find and select the best variables in a backward and forward stepwise fashon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Let's try fitting a binary logistic model using statsmodels to see if we can improve R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.165594\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:1029: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  184\n",
      "Model:                          Logit   Df Residuals:                      168\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Thu, 14 Jun 2018   Pseudo R-squ.:                  0.5476\n",
      "Time:                        14:18:01   Log-Likelihood:                -30.469\n",
      "converged:                      False   LL-Null:                       -67.355\n",
      "                                        LLR p-value:                 9.434e-10\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "prop          -0.5265        nan        nan        nan         nan         nan\n",
      "pop            0.0001   7.95e-05      1.423      0.155   -4.27e-05       0.000\n",
      "rob            1.4096      1.097      1.286      0.199      -0.740       3.559\n",
      "violent       -0.2191      0.481     -0.456      0.648      -1.161       0.723\n",
      "assul          0.0287      0.870      0.033      0.974      -1.676       1.733\n",
      "burg           0.4270        nan        nan        nan         nan         nan\n",
      "theft          0.5267        nan        nan        nan         nan         nan\n",
      "motor         -1.0254        nan        nan        nan         nan         nan\n",
      "prop2      -9.166e-05   6.97e-05     -1.315      0.189      -0.000     4.5e-05\n",
      "pop2       -1.609e-09   1.22e-09     -1.324      0.185   -3.99e-09    7.73e-10\n",
      "rob2          -0.0060      0.005     -1.296      0.195      -0.015       0.003\n",
      "violent2       0.0010      0.002      0.650      0.516      -0.002       0.004\n",
      "assul2        -0.0002      0.003     -0.060      0.952      -0.006       0.006\n",
      "burg2          0.0006      0.000      1.229      0.219      -0.000       0.001\n",
      "theft2         0.0001   8.38e-05      1.290      0.197   -5.61e-05       0.000\n",
      "mortor2        0.0070      0.007      0.946      0.344      -0.007       0.021\n",
      "intercept    -28.0304     59.430     -0.472      0.637    -144.511      88.450\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "# Declare predictors.\n",
    "X_statsmod = X_train2.copy()\n",
    "\n",
    "# The Statsmodels formulation requires a column with constant value 1 that\n",
    "# will act as the intercept.\n",
    "X_statsmod['intercept'] = 1 \n",
    "\n",
    "# Declare and fit the model.\n",
    "logit = sm.Logit(Y_train, X_statsmod)\n",
    "result = logit.fit()\n",
    "\n",
    "# Lots of information about the model and its coefficients, but the\n",
    "# accuracy rate for predictions is missing.\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184, 1), (184,))"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the output arrays for tabular form so can use table function \n",
    "Y_train.shape, pred_y_statsmod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1D = Y_train.ravel()  # We need to convert Y Train 2D to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy by admission status\n",
      "col_0    0   1\n",
      "row_0         \n",
      "0      160   2\n",
      "1        8  14\n",
      "\n",
      " Percentage accuracy\n",
      "0.945652173913\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy. First, get probability that each row will be admitted.\n",
    "pred_statsmod = result.predict(X_statsmod)\n",
    "\n",
    "# Code admission as 1 if probability is greater than .5.\n",
    "pred_y_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Accuracy table.\n",
    "table = pd.crosstab(Y_train_1D, pred_y_statsmod)\n",
    "\n",
    "print('\\n Accuracy by admission status')\n",
    "print(table)\n",
    "print('\\n Percentage accuracy')\n",
    "print((table.iloc[0,0] + table.iloc[1,1]) / (table.sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of the model is very high but becasue of class imbalance the result is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184, 1), (184, 16), (184, 16))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_test2.shape, X_train2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Below we do the same using SKlearn LogisticRegression method and this time try to manipualte  \"penalty\" argument of L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=-1, penalty='l2', random_state=0,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'Cs': [10, 100, 500]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "# Create a classifier object\n",
    "lr = LogisticRegressionCV(penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "params = {'Cs':[10, 100, 500]}\n",
    "\n",
    "\n",
    "# instantiate a gridsearch class\n",
    "grid = GridSearchCV(lr, params)\n",
    "\n",
    "# Train model\n",
    "grid.fit(X_train2, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cs': 10}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the best params\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We tried multiple penalty parameters and it seems C = 10 is the best one when using L2.\n",
    "## We cannot run the same LogisticRegressionCV for L1 as this SKlearn module does not suupport it. But we can try couple of C manually.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880434782609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object\n",
    "logistic_regression = LogisticRegression(C=10, penalty='l1')\n",
    "\n",
    "# Train model\n",
    "model1 = logistic_regression.fit(X_train2, Y_train)\n",
    "# print('\\nR-squared complex model:')\n",
    "print(model1.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880434782609\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object\n",
    "logistic_regression = LogisticRegression(C=1000, penalty='l1')\n",
    "\n",
    "# Train model\n",
    "model1 = logistic_regression.fit(X_train2, Y_train)\n",
    "# print('\\nR-squared complex model:')\n",
    "print(model1.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It seems L1 method for this data set is not sensitive with different level of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
